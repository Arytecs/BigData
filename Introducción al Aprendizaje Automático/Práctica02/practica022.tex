\documentclass[12pt,twoside]{article}

% La extensión total de la memoria deberá ser de un máximo de 50 páginas (excluidos resumen, índice y posibles anexos).

% Según las recomendaciones de estilo, el formato de la memoria se ajustará a lo siguiente:
% ? Formato del papel: DIN A4.
% ? Impresión a dos caras.
% ? Márgenes: superior e inferior, 2.5 cm. Márgenes laterales: páginas impares, izquierdo 4 cm y derecho 2 cm; páginas % pares, izquierdo 2 cm y derecho 4 cm.
% ? Tipo de letra: Times New Roman de 12 puntos.
% ? Interlineado: 1.5 líneas.
% ? Alineación: justificación completa.
% ? Sangrado de párrafo: 0.5 cm la primera línea de cada párrafo. No se
%pondrá espacio entre párrafos.
% ? Las páginas deberán ir numeradas en números arábigos.

% Teniendo en cuenta las indicaciones previas, definimos el estilo en LaTeX:

% Indicaciones para el idioma:
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{float}

% Adaptación de itemize y enumerate a los usos tipograficos españoles:
\let\layoutspanish\relax
\addto\captionsspanish{\def\tablename{Tabla}} % para que escriba "Tabla" en lugar de "Cuadro"
\unaccentedoperators  % para que no acentúe los operadores

% Área de impresión de una página:
\usepackage[a4paper]{geometry}
  \geometry{hmargin={2.5cm,2.5cm},height=22cm}

% Formato de algunas distancias:
\renewcommand{\baselinestretch}{1.2}    % separación entre líneas de un mismo párrafo
\setlength{\partopsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\topsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0.25\baselineskip}   % separación entre párrafos

\renewcommand{\textfraction}{0.1}   % mínima fracción de la página para el texto
\renewcommand{\topfraction}{1}      % máxima fracción de la página para objetos flotantes en la parte superior
\renewcommand{\bottomfraction}{1}
\renewcommand{\floatpagefraction}{1}

\setcounter{totalnumber}{5}
\setcounter{topnumber}{3}
\setcounter{bottomnumber}{2}

% Adaptación de las "caption" de los entorns "figure" y "table":
\usepackage{caption}
\setcaptionwidth{\textwidth}
\addtolength{\captionwidth}{-2\parindent}
\captionsetup{margin=\leftmargini,%
  width=\captionwidth,%
  labelfont={up,bf},%
  font={small,sl},%
  %indention={\captionindent
}

% Indentación del primer párrafo de una sección:
\usepackage{indentfirst}

% Definición del color grisclaro en la salida PDF:
\usepackage[pdftex]{color}

% Gráficos:
\usepackage[pdftex]{graphicx}

% Paquetes recomendados para la inclusión de fórmulas matemáticas:
\usepackage{amsmath}
\allowdisplaybreaks  % para que pueda partir fórmulas que ocupan más de una línea, necesita el paquete anterior
\usepackage{amssymb} % para cargar algunos símbolos como \blacksquare y \square
\usepackage{amsfonts} % para cargar algunas fuentes en estilo matemático
\usepackage{enumerate}
% Teoremas (se pueden definir todos los que se necesiten):

\newtheorem{theorem}{Teorema}[section]
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{definition}[theorem]{Definición}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{app}[theorem]{Aplicación}
\newtheorem{remark}[theorem]{Observación}
\newtheorem{agrad}[theorem]{Agradecimiento}
\newtheorem{algo}[theorem]{Algoritmo}
\newtheorem{axiom}[theorem]{Axioma}
\newtheorem{case}[theorem]{Caso}
\newtheorem{conclu}[theorem]{Conclusión}
\newtheorem{conjectura}[theorem]{Conjetura}
\newtheorem{notac}[theorem]{Notación}
\newtheorem{soluc}[theorem]{Solución}
\newtheorem{summary}[theorem]{Sumario}


\newtheorem{proof}[theorem]{Demostración.}
\renewenvironment{proof}{\emph{Demostración.}} {\quad \hfill $\blacksquare$ \newline} % para que aparezca un cuadrado negro al acabar la demostración


% Definición de cabeceras y pies de página:

\usepackage{fancyhdr}                     % para definir distintos tipos de cabeceras y pies de página

\newcommand{\RunningAuthor}{Jose Joaquín Rodríguez García y Araceli Teruel Domenech}
\newcommand{\Author}[1]{\renewcommand{\RunningAuthor}{#1}}
\renewcommand{\leftmark}{\RunningAuthor}

\newcommand{\RunningTitle}{Práctica 2 Introducción al Aprendizaje Automático}
\newcommand{\Title}[1]{\renewcommand{\RunningTitle}{#1}}
\renewcommand{\rightmark}{\RunningTitle}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\small \slshape \leftmark}    % lo que aparece en la parte izquierda de la páginas impares
\fancyhead[RE]{\small \slshape \rightmark}   % lo que aparece en la parte derecha de las páginas pares
\fancyhead[RO,LE]{\small \slshape \thepage}  % el número de página aparece en la parte exterior de la cabecera

\renewcommand{\headrulewidth}{0.6pt}         % grueso de la línea horizontal por debajo de la cabecera de la página
\renewcommand{\footrulewidth}{0pt}           % grueso de la línea horizontal por encima del pie de página
                                             % en este caso está vacío
\setlength{\headheight}{1.5\headheight}      % aumenta la altura de la cabecera en una parte y media

\fancypagestyle{plain}{%                     % redefinición del estilo de página 'plain'
  \fancyhf{}                                 % limpia todas las cabeceras y pies de página
  \setlength{\headwidth}{\textwidth}
  \fancyfoot[C]{\small \slshape \thepage}    % excepto el centro del pie de página
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
  }

% Instrucciones que se usan frecuentemente
\newcommand{\abs}[1]{\ensuremath{|#1|}}

% Datos del trabajo y autor:
\title{Trabajo en \textit{Python}}
\author{Jose Joaquín Rodríguez García y Araceli Teruel Domenech\\*[1em]
\begin{minipage}{0.75\textwidth}
\footnotesize \itshape
\begin{center}
Universidad Politécnica de Valencia \\
Master en Big Data Analytics
\end{center}
\end{minipage}
}
\date{Octubre 2017}

% Para incluir paginas de otro pdf (por ejemplo, la de la portada):
\usepackage{pdfpages}














\begin{document}

\maketitle


\section{Primera Tarea}

Comparación de los tiempos de ejecución entre código puro de \textit{Python} y código usando la librería \textit{Numpy}.

\subsection{Enunciado}

\noindent
Esta tarea es un simple primer paso para ver como cargar un dataset y
cómo visualizarlo mediante herramientas gráficas tras reducir la dimensiona-
lidad mediante Principal Component Analysis (PCA).

Descargad de la Web de scikit-learn el ejemplo plot iris dataset.py
y ejecutadlo:
\begin{center}
\textit{python plot_iris_dataset.py}
\end{center}

Después lo mismo con

\begin{center}
\textit{python nb_iris_dataset.py}
\end{center}

pero tomando el fichero modificado para esta práctica disponible en Polifor-
maT.
En este ejemplo se ve cómo crear un objeto de la clase GaussianNB, cómo
utilizarlo para entrenar a partir del dataset y cómo comprobar la precisión
de la predicción sobre el mismo dataset.
En realidad esto no es una buena práctica, no se puede saber la capacidad
del modelo para generalizar. La precisión sobre el subcorpus de entrenamiento
no es una buena medida de la calidad, en este caso, del clasificador.


\subsection{Resolución}

Estudiando el código del primer archivo \textit{plot_iris_dataset.py} vemos cómo carga un dataset, le aplica un PCA con 3 componentes y lo muestra gráficamente. 
En el segundo archivo \textit{nb_iris_dataset.py} carga un dataset "MNIST original" y le aplica un Gaussian Naive Bayes con todos los datos cargados

%PONER CON CÓDIGO%
mnist = fetch_mldata( 'MNIST original' ) #, data_home='data' )

gnb = GaussianNB()

gnb.fit( mnist.data, mnist.target )

Y usando los mismos datos cargados, y con los que hemos entrenado, hacemos la predicción. Tras ejecutarlo varias veces, podemos comprobar que los resultados no varían:

%PONER CON CÓDIGO o capturar pantalla% 
6 muestras mal clasificadas de 150
Accuracy = 96.0%

Esto es porque estamos usando la misma muestra tanto para training como para test, lo cual no es una buena práctica.

\section{Segunda Tarea}

\subsection{Enunciado}

\noindent
Estudiad el código de load mnist.py para ver un ejemplo de cómo des-
cargar y guardar en caché un dataset de los que hay disponibles para utilizar
en scikit-learn y otros toolkits de Machine Learning.
Los datos se descargan desde http://mldata.org y se guardan en un
directorio por defecto a partir del HOME del usuario. Se puede comprobar tras
ejecutar la descarga:

\begin{center}
python load_mnist.py
ls -lR ~/scikit_learn_data
\end{center}
\begin{enumerate}[(a)]


\subsection{Resolución}



\section{Tercera Tarea}

\subsection{Enunciado}

\noindent
En esta tarea también vamos a utilizar un clasificador Gaussian Naive
Bayes sobre el dataset MNIST. En primer lugar ejecutaremos
\begin{center}
python nb_mnist_dataset.py
\end{center}
para comprobar cómo se comporta dicho clasificador en este dataset. Pero
claro, no es concluyente trabajar con todo el corpus para entrenamiento y
para test. En el siguiente ejemplo se utilizan las primeras 60000 muestras
para entrenamiento y las últimas 10000 para test.
\begin{center}
python nb_mnist_dataset_2.py
\end{center}
Ambos ejemplos demuestran que un clasificador del tipo Naive Bayes no es
una buena opción cuando se trata de imágenes con valores enteros para indi-
car la intensidad de los pı́xeles. Con el siguiente ejemplo se puede visualizar
un dı́gito aleatorio y hacernos una idea de que como es cada muestra.
\begin{center}
python show_mnist.py
\end{center}

\subsection{Resolución}


Empezamos ejecutando el archivo \textit{nb_mnist_dataset.py} vemos una predicción usando un clasificador Gaussian Naive Bayes sobre el dataset MNIST pero usando todo el dataset tanto para training como para test. Vemos los resultados que obtenemos y son 30924 muestras mal clasificadas de 70000 con 55.8\% de precisión.
Pasando al siguiente archivo nb_mnist_dataset_2.py hacemos exactamente la misma predicción pero esta vez usando 60000 muestras iniciales para el entrenamiento y las 10000 restantes para el test

%CODIGO%
X_test = mnist.data[60000:]
Y_test = mnist.target[60000:]

Y mezclamos los datos
(X_train,Y_train) = shuffle( mnist.data[:60000], mnist.target[:60000] )

Los resultados obtenidos son 4442 muestras mal clasificadas de 10000 con un 55.8\% de precisión.

\section{Cuarta Tarea}

\subsection{Enunciado}

\noindent
Realizemos algunas transformaciones a los datos de entrada para reducir
la dimensionalidad y ver si podemos conseguir algún efecto positivo.
\begin{center}
python nb_mnist_dataset_3.py
\end{center}

Este programa realiza una simple transformación al mismo tiempo que
realiza una reducción de la dimensionalidad. Esta reducción consiste en trans-
formar un muestra que es una imagen de 28 × 28 a un vector de 56 dimensio-
nes. Las primeras 28 componentes de cada nuevo vector contienen la suma
de las intensidades fila por fila, y las siguientes 28 la suma de la intensidades
columna por columna. Es una representación alternativa de la imagen.
¿Qué resultados obtenemos ahora? ¿Mejores o peores que antes? ¿Se nos
ocurre alguna otra operación para mejorar los resultados?
Mientras se nos ocurre algo para mejorar estudiemos la idea de la valida-
ción cruzada (cross-validation) para seleccionar el modelo que mejor resulta-
dos nos da en la validación para comprobar su validez con el de test.

\begin{center}
python nb_mnist_dataset_3_cv.py
\end{center}

Tras ejecutar este último programa Python vemos como, además de ba-
rajar las muestras al principio y apartar una partición o subcorpus para test,
ahora utilizamos otra partición para validación. De manera que una parte de
la partición de entrenamiento no se utiliza para aprender o estimar el modelo,
se utiliza para validarlo, es decir, para comprobar como se comporta. Si este
proceso lo repetimos varias veces (10 en el ejemplo) y seleccionamos aquél
modelo que mejor resultados ha dado con la validación, podemos esperar que
será el que mejor resultados nos dará con la partición de test, aunque no es
seguro al 100 \%.

\subsection{Resolución}

Podemos comprobar que reduciendo dimensión sí que obtenemos una mejora en los resultados:
3003 muestras mal clasificadas de 10000
Accuracy = 70.0%

Usando Cross Validation a estos datos con dimensiones reducidas vemos que la mejora no es mayor

3003 muestras mal clasificadas de 10000
Accuracy = 70.0%

\section{Quinta Tarea}

\subsection{Enunciado}
\noindent
Aplicad Principal Component Analysis (PCA) para reducir la dimensio-
nalidad.
Tomando como ejemplo lo visto en la tarea 1, modificad el código de
nb_mnist_dataset_3_cv.py y realizad una transformación a la variable mul-
tidimensional de entrada. No a la variable salida o target.

\subsection{Resolución}
Haciendo un PCA a las variables de la siguiente manera:
X_train = PCA(n_components=15).fit_transform(X_train)
X_test = PCA(n_components=15).fit_transform(X_test)

y ejecutando nuevamente el fichero obtenemos unos resultados bastante peores pero en tiempo de ejecución es bastante mejor
4726 muestras mal clasificadas de 10000
Accuracy = 52.7%



\subsection{Conclusión}
Como hemos podido comprobar, siempre que usemos la librería \textit{Numpy} para todo el cálculo será mucho más eficiente en tiempo de ejecución. Pero si vamos a usar código puro de \textit{Python} para realizar algún cálculo, es más eficaz, en cuanto a tiempo de computo, el definir las variables que vayamos a usar usando la librería \textit{Numpy}, ya que la diferencia es bastante notable en todos los casos. Por último comentar que el código puro de \textit{Python} para un número reducido de elementos (un número menor o igual a $100$) no presenta diferencias significativas, cuanto a tiempo de computo, con el resto de los \textit{scripts} usados.


\end{document} 